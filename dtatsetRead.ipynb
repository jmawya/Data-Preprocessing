{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6a7f14-9722-41a4-97d7-89727d672425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6b18ce-b470-4809-aa11-ada9fcf3a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jannatul Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5467da72-de06-433b-bf58-4148deb16d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "         id  Insult             Date  \\\n",
      "0        1       0  20120603163526Z   \n",
      "1        2       1  20120531215447Z   \n",
      "2        3       1  20120823164228Z   \n",
      "3        4       1  20120826010752Z   \n",
      "4        5       1  20120602223825Z   \n",
      "...    ...     ...              ...   \n",
      "2230  2231       0  20120528100303Z   \n",
      "2231  2232       1  20120531185813Z   \n",
      "2232  2233       0  20120529130822Z   \n",
      "2233  2234       1  20120531045826Z   \n",
      "2234  2235       0  20120531184524Z   \n",
      "\n",
      "                                                Comment        Usage  \n",
      "0                    \"like this if you are a tribe fan\"  PrivateTest  \n",
      "1                 \"you're idiot.......................\"  PrivateTest  \n",
      "2     \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
      "3     \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
      "4     \"haha green me red you now loser whos winning ...  PrivateTest  \n",
      "...                                                 ...          ...  \n",
      "2230  \"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...  PrivateTest  \n",
      "2231  \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...  PrivateTest  \n",
      "2232  \"sweetie pie is looking very much like her cou...  PrivateTest  \n",
      "2233  \"ball4real where are you with your miami g-ayn...  PrivateTest  \n",
      "2234  \"Man....if you are a 3 point shooter, you must...  PrivateTest  \n",
      "\n",
      "[2235 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Cyberbullying Thesis\\impermium_verification_labels.csv\")\n",
    "df.head()\n",
    "print(\"Dataset:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d04913-ddd7-4874-a277-a040d80703b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR FROM HIS BAT NICE STUPID\"\n",
      "2nd comment:\n",
      " \"you're idiot.......................\"\n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[3][\"Comment\"]\n",
    "print(st)\n",
    "\n",
    "st=df.iloc[1][\"Comment\"]\n",
    "print(\"2nd comment:\\n\",st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4706fd40-22a7-4606-99c6-26d802be1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'you', \"'re\", 'idiot', '.......................', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e033616-aa55-41ed-85b3-1f2e1b55572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"haha green me red you now loser whos winning now moron\"\n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[4][\"Comment\"]\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976939d5-4f03-4f46-8e13-142afefbd876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'haha', 'green', 'me', 'red', 'you', 'now', 'loser', 'whos', 'winning', 'now', 'moron', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a62d58-0aaa-4f5c-9041-9dc797433af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\nMe and God both hate-faggots.\\n\\nWhat's the difference between a-fag-and a refrigerator?\\nRefrigerators don't fart when you put the meat in.\"\n"
     ]
    }
   ],
   "source": [
    "ab=df.iloc[5][\"Comment\"]\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15c18d7-21ec-4dbc-97ba-92212ef89aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', '\\\\nme', 'and', 'god', 'both', 'hate-faggots.\\\\n\\\\nwhat', \"'s\", 'the', 'difference', 'between', 'a-fag-and', 'a', 'refrigerator', '?', '\\\\nrefrigerators', 'do', \"n't\", 'fart', 'when', 'you', 'put', 'the', 'meat', 'in', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(ab.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "529109d7-0a62-4712-bd19-2371735c3b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 25 samples and 26 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_disk=FreqDist(tokens)# it creates a dictionary list \n",
    "print(freq_disk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a3de95-373b-4129-b8eb-e87e9878e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``1\n",
      "\\nme1\n",
      "and1\n",
      "god1\n",
      "both1\n",
      "hate-faggots.\\n\\nwhat1\n",
      "'s1\n",
      "the2\n",
      "difference1\n",
      "between1\n",
      "a-fag-and1\n",
      "a1\n",
      "refrigerator1\n",
      "?1\n",
      "\\nrefrigerators1\n",
      "do1\n",
      "n't1\n",
      "fart1\n",
      "when1\n",
      "you1\n",
      "put1\n",
      "meat1\n",
      "in1\n",
      ".1\n",
      "''1\n"
     ]
    }
   ],
   "source": [
    "for word, freq in freq_disk.items():\n",
    "    print(word + str(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a9787f-d806-42dc-bcdd-638edae95abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Oh go kiss the ass of a goat....and you DUMMYCRAPS insult veterans....even YOUR Coward-Incompetent-Chief.....everyone of YOU ARE the enemy....and you'll ALL fall.....and hard too BOY.\"\n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[6][\"Comment\"]\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bd6883-c9f6-4fcc-b6e0-95f2a994747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'oh', 'go', 'kiss', 'the', 'ass', 'of', 'a', 'goat', '....', 'and', 'you', 'dummycraps', 'insult', 'veterans', '....', 'even', 'your', 'coward-incompetent-chief', '.....', 'everyone', 'of', 'you', 'are', 'the', 'enemy', '....', 'and', 'you', \"'ll\", 'all', 'fall', '.....', 'and', 'hard', 'too', 'boy', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32eef218-87a3-40de-bb41-b8f5c5daf82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 30 samples and 39 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist=FreqDist(tokens) #it creates frequency dictionary\n",
    "print(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7ed773-6d00-4d4c-9f6b-8a5fc4477ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` 1\n",
      "oh 1\n",
      "go 1\n",
      "kiss 1\n",
      "the 2\n",
      "ass 1\n",
      "of 2\n",
      "a 1\n",
      "goat 1\n",
      ".... 3\n",
      "and 3\n",
      "you 3\n",
      "dummycraps 1\n",
      "insult 1\n",
      "veterans 1\n",
      "even 1\n",
      "your 1\n",
      "coward-incompetent-chief 1\n",
      "..... 2\n",
      "everyone 1\n",
      "are 1\n",
      "enemy 1\n",
      "'ll 1\n",
      "all 1\n",
      "fall 1\n",
      "hard 1\n",
      "too 1\n",
      "boy 1\n",
      ". 1\n",
      "'' 1\n"
     ]
    }
   ],
   "source": [
    "#freq_Dist store dictionary and items returns the pair of data from the dictionary\n",
    "for word, freq in freq_dist.items(): \n",
    "    print(word , freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b631c567-7adc-44cb-a419-b886a0f48bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'oh', 'go', 'kiss', 'ass', 'goat', '....', 'dummycraps', 'insult', 'veterans', '....', 'even', 'coward-incompetent-chief', '.....', 'everyone', 'enemy', '....', \"'ll\", 'fall', '.....', 'hard', 'boy', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [\n",
    "    word for word in tokens\n",
    "    if word not in stop_words\n",
    "]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1433e2fe-071e-49e5-bbea-b0046cb13207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'not', 't', \"it'd\", \"they've\", 'he', 'off', 'once', 'then', \"haven't\", \"should've\", \"we've\", 'her', 'be', 'they', 'theirs', 'had', \"you've\", 'his', 'don', 'hasn', \"she's\", 'been', 'how', 'm', 'haven', 'when', 'why', 'same', \"that'll\", \"he'll\", 'hers', 'themselves', 'while', 'being', 'himself', 'because', 'weren', \"wouldn't\", 'were', \"weren't\", 'other', 'no', \"needn't\", 'who', 'through', 'up', 'from', \"hadn't\", 'do', 'in', 'them', 'which', 'can', 'ourselves', 'where', 'isn', 'mightn', 'over', \"i've\", \"they're\", 'ours', 'hadn', 'won', 'some', \"we'll\", 'own', \"don't\", 'him', 'into', 'so', 'has', 'd', 'she', 'doesn', 'below', 'that', 'any', 'if', 'an', 'll', 'wouldn', \"i'm\", 'more', 'as', 'what', 'most', 'for', 'our', 'each', 'and', 'with', 'its', 'will', 'aren', 'further', 'these', 'doing', 'was', \"shouldn't\", 'itself', 'does', 'myself', 'whom', 'until', 'are', \"he'd\", 'but', 'too', 'a', 'both', 'couldn', 'it', \"shan't\", 'very', \"they'd\", 'yours', 'me', 'y', 'against', 'there', \"you'll\", 'out', 'before', 'did', 'mustn', 'than', 'down', \"you'd\", 'didn', 'o', 'few', 'between', 'here', 'yourselves', 'only', \"we're\", 'of', 'about', 'or', 'again', 'your', \"she'd\", 'during', 'to', 'have', \"couldn't\", 're', \"mightn't\", \"aren't\", \"wasn't\", 'now', 'we', \"we'd\", \"isn't\", 'under', \"mustn't\", 'shouldn', \"you're\", 'such', 'above', 'my', \"it'll\", \"won't\", 'should', \"it's\", \"they'll\", \"hasn't\", 'i', 's', 'needn', 'yourself', \"doesn't\", 'nor', 'their', \"didn't\", 'the', 'am', 'wasn', 'ain', 'shan', \"he's\", 'herself', 'by', 'just', 'you', \"i'll\", 'ma', 've', 'at', 'those', 'after', \"i'd\", 'all', 'is', 'this', 'having', \"she'll\"}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "487aac46-6e2b-46e4-8906-ef17398301f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'oh', 'go', 'kiss', 'ass', 'goat', '....', 'dummycraps', 'insult', 'veterans', '....', 'even', 'coward-incompetent-chief', '.....', 'everyone', 'enemy', '....', \"'ll\", 'fall', '.....', 'hard', 'boy', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []          # empty list to store valid words\n",
    "\n",
    "for word in tokens:           # loop through each token\n",
    "    if word not in stop_words:  # check if word is NOT a stop word\n",
    "        filtered_tokens.append(word)  # add word to the list\n",
    "print(filtered_tokens)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c76404-a50c-4f24-a946-ec9c730df392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('oh', 'RB'), ('go', 'VB'), ('kiss', 'VB'), ('the', 'DT'), ('ass', 'NN'), ('of', 'IN'), ('a', 'DT'), ('goat', 'NN'), ('....', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('dummycraps', 'VBP'), ('insult', 'JJ'), ('veterans', 'NNS'), ('....', 'VBD'), ('even', 'RB'), ('your', 'PRP$'), ('coward-incompetent-chief', 'JJ'), ('.....', 'NNP'), ('everyone', 'NN'), ('of', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('enemy', 'NN'), ('....', 'NN'), ('and', 'CC'), ('you', 'PRP'), (\"'ll\", 'MD'), ('all', 'VB'), ('fall', 'NN'), ('.....', 'NNP'), ('and', 'CC'), ('hard', 'RB'), ('too', 'RB'), ('boy', 'JJ'), ('.', '.'), (\"''\", \"''\")]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f253b319-a16a-44cb-8a03-aaef4ed0b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` → ``\n",
      "oh → oh\n",
      "go → go\n",
      "kiss → kiss\n",
      "the → the\n",
      "ass → as\n",
      "of → of\n",
      "a → a\n",
      "goat → goat\n",
      ".... → ....\n",
      "and → and\n",
      "you → you\n",
      "dummycraps → dummycraps\n",
      "insult → insult\n",
      "veterans → veteran\n",
      ".... → ....\n",
      "even → even\n",
      "your → your\n",
      "coward-incompetent-chief → coward-incompetent-chief\n",
      "..... → .....\n",
      "everyone → everyone\n",
      "of → of\n",
      "you → you\n",
      "are → are\n",
      "the → the\n",
      "enemy → enemy\n",
      ".... → ....\n",
      "and → and\n",
      "you → you\n",
      "'ll → 'll\n",
      "all → all\n",
      "fall → fall\n",
      "..... → .....\n",
      "and → and\n",
      "hard → hard\n",
      "too → too\n",
      "boy → boy\n",
      ". → .\n",
      "'' → ''\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for word in tokens:\n",
    "    base_word = lemmatizer.lemmatize(word)\n",
    "    print(word, \"→\", base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93268a9e-5446-4fd3-ae5a-c133aa813f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # verb\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))   # adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c73a199-f7e4-4ca3-8574-9d2d8290ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``-``\n",
      "oh-oh\n",
      "go-go\n",
      "kiss-kiss\n",
      "the-the\n",
      "ass-ass\n",
      "of-of\n",
      "a-a\n",
      "goat-goat\n",
      "....-....\n",
      "and-and\n",
      "you-you\n",
      "dummycraps-dummycraps\n",
      "insult-insult\n",
      "veterans-veterans\n",
      "....-....\n",
      "even-even\n",
      "your-your\n",
      "coward-incompetent-chief-coward-incompetent-chief\n",
      ".....-.....\n",
      "everyone-everyone\n",
      "of-of\n",
      "you-you\n",
      "are-be\n",
      "the-the\n",
      "enemy-enemy\n",
      "....-....\n",
      "and-and\n",
      "you-you\n",
      "'ll-'ll\n",
      "all-all\n",
      "fall-fall\n",
      ".....-.....\n",
      "and-and\n",
      "hard-hard\n",
      "too-too\n",
      "boy-boy\n",
      ".-.\n",
      "''-''\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in tokens:\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    print (word+'-'+lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1620bd2-efff-4e48-ac7a-79a64c071c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``           POS=``   → Base=``\n",
      "oh           POS=RB   → Base=oh\n",
      "go           POS=VB   → Base=go\n",
      "kiss         POS=VB   → Base=kiss\n",
      "the          POS=DT   → Base=the\n",
      "ass          POS=NN   → Base=as\n",
      "of           POS=IN   → Base=of\n",
      "a            POS=DT   → Base=a\n",
      "goat         POS=NN   → Base=goat\n",
      "....         POS=NN   → Base=....\n",
      "and          POS=CC   → Base=and\n",
      "you          POS=PRP  → Base=you\n",
      "dummycraps   POS=VBP  → Base=dummycraps\n",
      "insult       POS=JJ   → Base=insult\n",
      "veterans     POS=NNS  → Base=veteran\n",
      "....         POS=VBD  → Base=....\n",
      "even         POS=RB   → Base=even\n",
      "your         POS=PRP$ → Base=your\n",
      "coward-incompetent-chief POS=JJ   → Base=coward-incompetent-chief\n",
      ".....        POS=NNP  → Base=.....\n",
      "everyone     POS=NN   → Base=everyone\n",
      "of           POS=IN   → Base=of\n",
      "you          POS=PRP  → Base=you\n",
      "are          POS=VBP  → Base=be\n",
      "the          POS=DT   → Base=the\n",
      "enemy        POS=NN   → Base=enemy\n",
      "....         POS=NN   → Base=....\n",
      "and          POS=CC   → Base=and\n",
      "you          POS=PRP  → Base=you\n",
      "'ll          POS=MD   → Base='ll\n",
      "all          POS=VB   → Base=all\n",
      "fall         POS=NN   → Base=fall\n",
      ".....        POS=NNP  → Base=.....\n",
      "and          POS=CC   → Base=and\n",
      "hard         POS=RB   → Base=hard\n",
      "too          POS=RB   → Base=too\n",
      "boy          POS=JJ   → Base=boy\n",
      ".            POS=.    → Base=.\n",
      "''           POS=''   → Base=''\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 1: Map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ      # adjective\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB     # verb\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN     # noun\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV      # adverb\n",
    "    else:\n",
    "        return wordnet.NOUN    # default fallback\n",
    "\n",
    "# Step 2: Your word list\n",
    "words = [\"running\", \"better\", \"cars\", \"studies\", \"quickly\", \"children\"]\n",
    "\n",
    "# Step 3: POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Step 4: Lemmatize using POS\n",
    "base_words = []\n",
    "\n",
    "for word, pos in pos_tags:\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    base_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "    base_words.append((word, pos, base_word))\n",
    "\n",
    "# Step 5: Print results\n",
    "for original, pos, base in base_words:\n",
    "    print(f\"{original:<12} POS={pos:<4} → Base={base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bd7f366-4b9d-4b77-9e0e-29f9c14a597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jannatul Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "Detected TEXT column : Text\n",
      "Detected LABEL column: oh_label\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy: 0.9235748070699527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      3562\n",
      "           1       0.75      0.49      0.59       455\n",
      "\n",
      "    accuracy                           0.92      4017\n",
      "   macro avg       0.84      0.73      0.78      4017\n",
      "weighted avg       0.92      0.92      0.92      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3487   75]\n",
      " [ 232  223]]\n",
      "\n",
      "Random Forest\n",
      "Accuracy: 0.9317898929549415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      3562\n",
      "           1       0.78      0.56      0.65       455\n",
      "\n",
      "    accuracy                           0.93      4017\n",
      "   macro avg       0.86      0.77      0.81      4017\n",
      "weighted avg       0.93      0.93      0.93      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3489   73]\n",
      " [ 201  254]]\n",
      "\n",
      "Naive Bayes\n",
      "Accuracy: 0.9168533731640528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95      3562\n",
      "           1       0.78      0.37      0.50       455\n",
      "\n",
      "    accuracy                           0.92      4017\n",
      "   macro avg       0.85      0.68      0.73      4017\n",
      "weighted avg       0.91      0.92      0.90      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3516   46]\n",
      " [ 288  167]]\n",
      "\n",
      "Voting Ensemble\n",
      "Accuracy: 0.932536718944486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      3562\n",
      "           1       0.80      0.54      0.65       455\n",
      "\n",
      "    accuracy                           0.93      4017\n",
      "   macro avg       0.87      0.76      0.80      4017\n",
      "weighted avg       0.93      0.93      0.93      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3499   63]\n",
      " [ 208  247]]\n",
      "\n",
      "AdaBoost\n",
      "Accuracy: 0.9121234752302714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      3562\n",
      "           1       0.63      0.56      0.59       455\n",
      "\n",
      "    accuracy                           0.91      4017\n",
      "   macro avg       0.78      0.76      0.77      4017\n",
      "weighted avg       0.91      0.91      0.91      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3410  152]\n",
      " [ 201  254]]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Imports & NLTK setup\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------------\n",
    "# NLTK downloads (run once)\n",
    "# -------------------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize tools\n",
    "# -------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# POS tag mapping\n",
    "# -------------------------------\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# ===============================\n",
    "# Load dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Thesis\\cyberbullying.csv\"\n",
    ")\n",
    "\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "\n",
    "# ===============================\n",
    "# Auto-detect text & label\n",
    "# ===============================\n",
    "text_col = None\n",
    "label_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object and text_col is None:\n",
    "        text_col = col\n",
    "    elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "        label_col = col\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise ValueError(\"Could not detect text/label columns\")\n",
    "\n",
    "print(f\"Detected TEXT column : {text_col}\")\n",
    "print(f\"Detected LABEL column: {label_col}\")\n",
    "\n",
    "df = df[[text_col, label_col]].copy()\n",
    "df.columns = ['comment', 'insult']\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df['insult'] = df['insult'].astype(int)\n",
    "\n",
    "# ===============================\n",
    "# Text preprocessing (LEMMATIZATION)\n",
    "# ===============================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|<.*?>', ' ', str(text).lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "df['clean_comment'] = df['comment'].apply(clean_text)\n",
    "\n",
    "# ===============================\n",
    "# Vectorization\n",
    "# ===============================\n",
    "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['clean_comment'])\n",
    "y = df['insult']\n",
    "\n",
    "# ===============================\n",
    "# Train-test split\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Models\n",
    "# ===============================\n",
    "lr = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "nb = MultinomialNB()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    estimator=LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# Evaluation\n",
    "# ===============================\n",
    "for name, model in [\n",
    "    ('Logistic Regression', lr),\n",
    "    ('Random Forest', rf),\n",
    "    ('Naive Bayes', nb),\n",
    "    ('Voting Ensemble', voting),\n",
    "    ('AdaBoost', adaboost)\n",
    "]:\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1f044-2292-473f-bcf8-23a5f9cdc6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22857a-775d-472f-82e4-b523cfed726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fb093-ddab-404f-ae2a-7586fce42712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
