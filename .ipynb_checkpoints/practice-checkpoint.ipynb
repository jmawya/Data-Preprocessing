{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "27e10f8d-2435-4d5d-a3a5-cd6c357d2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "55b110e9-d6d1-4705-9152-351fc980e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4c0a8cd8-78e8-45da-9701-946a4a4d9d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Insult', 'Date', 'Comment', 'Usage']\n",
      "   id  Insult             Date  \\\n",
      "0   1       0  20120603163526Z   \n",
      "1   2       1  20120531215447Z   \n",
      "2   3       1  20120823164228Z   \n",
      "3   4       1  20120826010752Z   \n",
      "4   5       1  20120602223825Z   \n",
      "\n",
      "                                             Comment        Usage  \n",
      "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
      "1              \"you're idiot.......................\"  PrivateTest  \n",
      "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
      "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
      "4  \"haha green me red you now loser whos winning ...  PrivateTest  \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Thesis\\impermium_verification_labels.csv\")\n",
    "print(df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b081edf-30cf-4587-aca9-173a75b098df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"like this if you are a tribe fan\"\n",
      "\"you're idiot.......................\"\n",
      "\"I am a woman Babs, and the only \"war on women\" I see is coming from jackazzes like you.\\xa0 I don't need your protection or your ignorant rhetoric masquerading as representing my best interests.\\xa0 \"\n",
      "\"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR FROM HIS BAT NICE STUPID\"\n",
      "\"haha green me red you now loser whos winning now moron\"\n",
      "\"\\nMe and God both hate-faggots.\\n\\nWhat's the difference between a-fag-and a refrigerator?\\nRefrigerators don't fart when you put the meat in.\"\n"
     ]
    }
   ],
   "source": [
    "st1=df.iloc[0][\"Comment\"]\n",
    "print(st1)\n",
    "st2=df.iloc[1][\"Comment\"]\n",
    "print(st2)\n",
    "st3=df.iloc[2][\"Comment\"]\n",
    "print(st3)\n",
    "st4=df.iloc[3][\"Comment\"]\n",
    "print(st4)\n",
    "st5=df.iloc[4][\"Comment\"]\n",
    "print(st5)\n",
    "st6=df.iloc[5][\"Comment\"]\n",
    "print(st6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "59393b93-b140-4ffa-b4af-862a8ca722c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'like', 'this', 'if', 'you', 'are', 'a', 'tribe', 'fan', \"''\"]\n",
      "['``', 'you', \"'re\", 'idiot', '.......................', \"''\"]\n",
      "['``', 'i', 'am', 'a', 'woman', 'babs', ',', 'and', 'the', 'only', '``', 'war', 'on', 'women', \"''\", 'i', 'see', 'is', 'coming', 'from', 'jackazzes', 'like', 'you.\\\\xa0', 'i', 'do', \"n't\", 'need', 'your', 'protection', 'or', 'your', 'ignorant', 'rhetoric', 'masquerading', 'as', 'representing', 'my', 'best', 'interests.\\\\xa0', '``']\n",
      "['``', 'wow', '&', 'you', 'benefitted', 'so', 'many', 'wins', 'this', 'year', 'from', 'his', 'bat', 'nice', 'stupid', \"''\"]\n",
      "['``', 'haha', 'green', 'me', 'red', 'you', 'now', 'loser', 'whos', 'winning', 'now', 'moron', \"''\"]\n",
      "['``', '\\\\nme', 'and', 'god', 'both', 'hate-faggots.\\\\n\\\\nwhat', \"'s\", 'the', 'difference', 'between', 'a-fag-and', 'a', 'refrigerator', '?', '\\\\nrefrigerators', 'do', \"n't\", 'fart', 'when', 'you', 'put', 'the', 'meat', 'in', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "tokens1=word_tokenize(st1.lower())\n",
    "print(tokens1)\n",
    "tokens2=word_tokenize(st2.lower())\n",
    "print(tokens2)\n",
    "tokens3=word_tokenize(st3.lower())\n",
    "print(tokens3)\n",
    "tokens4=word_tokenize(st4.lower())\n",
    "print(tokens4)\n",
    "tokens5=word_tokenize(st5.lower())\n",
    "print(tokens5)\n",
    "\n",
    "\n",
    "tokens6=word_tokenize(st6.lower())\n",
    "print(tokens6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1fc4047-280a-4180-828f-dfc8922f2bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 10 samples and 10 outcomes>\n",
      "`` 1\n",
      "like 1\n",
      "this 1\n",
      "if 1\n",
      "you 1\n",
      "are 1\n",
      "a 1\n",
      "tribe 1\n",
      "fan 1\n",
      "'' 1\n",
      "<FreqDist with 6 samples and 6 outcomes>\n",
      "2nd one:\n",
      " `` 1\n",
      "2nd one:\n",
      " you 1\n",
      "2nd one:\n",
      " 're 1\n",
      "2nd one:\n",
      " idiot 1\n",
      "2nd one:\n",
      " ....................... 1\n",
      "2nd one:\n",
      " '' 1\n",
      "<FreqDist with 35 samples and 40 outcomes>\n",
      "`` 3\n",
      "i 3\n",
      "am 1\n",
      "a 1\n",
      "woman 1\n",
      "babs 1\n",
      ", 1\n",
      "and 1\n",
      "the 1\n",
      "only 1\n",
      "war 1\n",
      "on 1\n",
      "women 1\n",
      "'' 1\n",
      "see 1\n",
      "is 1\n",
      "coming 1\n",
      "from 1\n",
      "jackazzes 1\n",
      "like 1\n",
      "you.\\xa0 1\n",
      "do 1\n",
      "n't 1\n",
      "need 1\n",
      "your 2\n",
      "protection 1\n",
      "or 1\n",
      "ignorant 1\n",
      "rhetoric 1\n",
      "masquerading 1\n",
      "as 1\n",
      "representing 1\n",
      "my 1\n",
      "best 1\n",
      "interests.\\xa0 1\n",
      "<FreqDist with 16 samples and 16 outcomes>\n",
      "`` 1\n",
      "wow 1\n",
      "& 1\n",
      "you 1\n",
      "benefitted 1\n",
      "so 1\n",
      "many 1\n",
      "wins 1\n",
      "this 1\n",
      "year 1\n",
      "from 1\n",
      "his 1\n",
      "bat 1\n",
      "nice 1\n",
      "stupid 1\n",
      "'' 1\n",
      "<FreqDist with 12 samples and 13 outcomes>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FreqDist' object has no attribute 'intems'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m freq_dist=FreqDist(tokens5)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(freq_dist)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word,repeat \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfreq_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintems\u001b[49m():\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(word,repeat)\n",
      "\u001b[31mAttributeError\u001b[39m: 'FreqDist' object has no attribute 'intems'"
     ]
    }
   ],
   "source": [
    "freq_dist=FreqDist(tokens1)\n",
    "print(freq_dist)\n",
    "for word, freq in freq_dist.items():\n",
    "    print(word, freq)\n",
    "freq_dist=FreqDist(tokens2)\n",
    "print(freq_dist)\n",
    "for word,freq in freq_dist.items():\n",
    "    print(\"2nd one:\\n\",word, freq)\n",
    "freq_dist=FreqDist(tokens3)\n",
    "print(freq_dist)\n",
    "for word, freq in freq_dist.items():\n",
    "    print(word,freq)\n",
    "freq_dist=FreqDist(tokens4)\n",
    "print(freq_dist)\n",
    "for word, repeat in freq_dist.items():\n",
    "    print(word, repeat)\n",
    "freq_dist=FreqDist(tokens5)\n",
    "print(freq_dist)\n",
    "for word,repeat in freq_dist.intems():\n",
    "    print(word,repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c99b1bdc-fcd1-411a-9415-6d6f7eb48261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"like this if you are a tribe fan\"\n",
      "['``', 'like', 'tribe', 'fan', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "'''filtered_tokens=[word for word in tokens1\n",
    "                 if word not in stop_words\n",
    "                ]'''\n",
    "sd=df.iloc[0][\"Comment\"]\n",
    "print(sd)\n",
    "filtered_tokens=[]\n",
    "for word in tokens1:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "print(filtered_tokens)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1a0cef3b-b929-47f9-9495-5b0ff18dfb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'woman', 'babs', ',', '``', 'war', 'women', \"''\", 'see', 'coming', 'jackazzes', 'like', 'you.\\\\xa0', \"n't\", 'need', 'protection', 'ignorant', 'rhetoric', 'masquerading', 'representing', 'best', 'interests.\\\\xa0', '``']\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_tokens=[]\n",
    "for word in tokens3:\n",
    "    if word not in stop_words:\n",
    "     filtered_tokens.append(word)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4aa9fe2-05db-4658-b619-4746acde1b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', '\\\\nme', 'god', 'hate-faggots.\\\\n\\\\nwhat', \"'s\", 'difference', 'a-fag-and', 'refrigerator', '?', '\\\\nrefrigerators', \"n't\", 'fart', 'put', 'meat', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_tokens=[]\n",
    "for word in tokens6:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "print(filtered_tokens)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6bfef88b-c546-4fb1-ab8e-03ef948836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'haha', 'green', 'red', 'loser', 'whos', 'winning', 'moron', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))    \n",
    "filtered_tokens=[]\n",
    "for word in tokens5:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "print(filtered_tokens)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c66ad51a-688e-4df9-83a9-288869eca8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('you', 'PRP'), (\"'re\", 'VBP'), ('idiot', 'JJ'), ('.......................', 'NN'), (\"''\", \"''\")]\n",
      "[('``', '``'), ('like', 'IN'), ('this', 'DT'), ('if', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('tribe', 'JJ'), ('fan', 'NN'), (\"''\", \"''\")]\n"
     ]
    }
   ],
   "source": [
    "pos_tags=nltk.pos_tag(tokens2)\n",
    "print(pos_tags)\n",
    "pos=nltk.pos_tag(tokens1)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e4a92b7-7b58-492d-bd30-9061b4f3d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('haha', 'NN'), ('green', 'JJ'), ('me', 'PRP'), ('red', 'JJ'), ('you', 'PRP'), ('now', 'RB'), ('loser', 'VBP'), ('whos', 'NNS'), ('winning', 'VBG'), ('now', 'RB'), ('moron', 'NN'), (\"''\", \"''\")]\n",
      "tokens6:\n",
      " [('``', '``'), ('\\\\nme', 'NN'), ('and', 'CC'), ('god', 'VB'), ('both', 'DT'), ('hate-faggots.\\\\n\\\\nwhat', 'WDT'), (\"'s\", 'POS'), ('the', 'DT'), ('difference', 'NN'), ('between', 'IN'), ('a-fag-and', 'NN'), ('a', 'DT'), ('refrigerator', 'NN'), ('?', '.'), ('\\\\nrefrigerators', 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), ('fart', 'VB'), ('when', 'WRB'), ('you', 'PRP'), ('put', 'VBP'), ('the', 'DT'), ('meat', 'NN'), ('in', 'IN'), ('.', '.'), (\"''\", \"''\")]\n",
      "tokens4:\n",
      " [('``', '``'), ('wow', 'NN'), ('&', 'CC'), ('you', 'PRP'), ('benefitted', 'VBD'), ('so', 'RB'), ('many', 'JJ'), ('wins', 'NNS'), ('this', 'DT'), ('year', 'NN'), ('from', 'IN'), ('his', 'PRP$'), ('bat', 'NN'), ('nice', 'JJ'), ('stupid', 'NN'), (\"''\", \"''\")]\n",
      "tokens3:\n",
      " [('``', '``'), ('i', 'NN'), ('am', 'VBP'), ('a', 'DT'), ('woman', 'NN'), ('babs', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('only', 'JJ'), ('``', '``'), ('war', 'NN'), ('on', 'IN'), ('women', 'NNS'), (\"''\", \"''\"), ('i', 'NN'), ('see', 'VBP'), ('is', 'VBZ'), ('coming', 'VBG'), ('from', 'IN'), ('jackazzes', 'NNS'), ('like', 'IN'), ('you.\\\\xa0', 'NN'), ('i', 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), ('need', 'VB'), ('your', 'PRP$'), ('protection', 'NN'), ('or', 'CC'), ('your', 'PRP$'), ('ignorant', 'JJ'), ('rhetoric', 'NN'), ('masquerading', 'VBG'), ('as', 'IN'), ('representing', 'VBG'), ('my', 'PRP$'), ('best', 'JJS'), ('interests.\\\\xa0', 'NN'), ('``', '``')]\n"
     ]
    }
   ],
   "source": [
    "pos=nltk.pos_tag(tokens5)\n",
    "print(pos)\n",
    "pos=nltk.pos_tag(tokens6)\n",
    "print('tokens6:\\n',pos)\n",
    "pos=nltk.pos_tag(tokens4)\n",
    "print(\"tokens4:\\n\", pos)\n",
    "pos=nltk.pos_tag(tokens3)\n",
    "print(\"tokens3:\\n\",pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "25bbfff1-555a-47b1-a6fb-2987c98cd193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` → ``\n",
      "like → like\n",
      "this → this\n",
      "if → if\n",
      "you → you\n",
      "are → are\n",
      "a → a\n",
      "tribe → tribe\n",
      "fan → fan\n",
      "'' → ''\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "for word in tokens1:\n",
    "    base_word=lemmatizer.lemmatize(word)\n",
    "    print(word,\"→\",base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3ed5acd6-f5b8-425c-b8d7-9139b73eb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` = ``\n",
      "\\nme = \\nme\n",
      "and = and\n",
      "god = god\n",
      "both = both\n",
      "hate-faggots.\\n\\nwhat = hate-faggots.\\n\\nwhat\n",
      "'s = 's\n",
      "the = the\n",
      "difference = difference\n",
      "between = between\n",
      "a-fag-and = a-fag-and\n",
      "a = a\n",
      "refrigerator = refrigerator\n",
      "? = ?\n",
      "\\nrefrigerators = \\nrefrigerators\n",
      "do = do\n",
      "n't = n't\n",
      "fart = fart\n",
      "when = when\n",
      "you = you\n",
      "put = put\n",
      "the = the\n",
      "meat = meat\n",
      "in = in\n",
      ". = .\n",
      "'' = ''\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "for word in tokens6:\n",
    "    base_word=lemmatizer.lemmatize(word)\n",
    "    print(word,\"=\",base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "40996e9b-3c5c-4c6d-8ecd-c193d00224fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('wow', 'NN'), ('&', 'CC'), ('you', 'PRP'), ('benefitted', 'VBD'), ('so', 'RB'), ('many', 'JJ'), ('wins', 'NNS'), ('this', 'DT'), ('year', 'NN'), ('from', 'IN'), ('his', 'PRP$'), ('bat', 'NN'), ('nice', 'JJ'), ('stupid', 'NN'), (\"''\", \"''\")]\n",
      "``            n        ``\n",
      "wow           n        wow\n",
      "&             n        &\n",
      "you           n        you\n",
      "benefitted    v        benefit\n",
      "so            r        so\n",
      "many          a        many\n",
      "wins          n        win\n",
      "this          n        this\n",
      "year          n        year\n",
      "from          n        from\n",
      "his           n        his\n",
      "bat           n        bat\n",
      "nice          a        nice\n",
      "stupid        n        stupid\n",
      "''            n        ''\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "def gt(nlos):\n",
    "    if nlos.startswith('J'):\n",
    "        return wordnet.ADJ \n",
    "    elif nlos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nlos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nlos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "post=nltk.pos_tag(tokens4)\n",
    "print(post)\n",
    "\n",
    "for word,pos in post:\n",
    "    w_pos=gt(pos)\n",
    "    base_word=lemmatizer.lemmatize(word,w_pos)\n",
    "   # print(word,w_pos,base_word)\n",
    "    print(f\"{word:<14}{w_pos:<8} {base_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3ea4b004-846e-4800-95e7-0a98441321e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      " ['id', 'Insult', 'Date', 'Comment', 'Usage']\n",
      "0                      \"like this if you are a tribe fan\"\n",
      "1                   \"you're idiot.......................\"\n",
      "2       \"I am a woman Babs, and the only \"war on women...\n",
      "3       \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
      "4       \"haha green me red you now loser whos winning ...\n",
      "                              ...                        \n",
      "2230    \"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...\n",
      "2231    \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...\n",
      "2232    \"sweetie pie is looking very much like her cou...\n",
      "2233    \"ball4real where are you with your miami g-ayn...\n",
      "2234    \"Man....if you are a 3 point shooter, you must...\n",
      "Name: Comment, Length: 2235, dtype: object\n",
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2230    0\n",
      "2231    1\n",
      "2232    0\n",
      "2233    1\n",
      "2234    0\n",
      "Name: Insult, Length: 2235, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Thesis\\impermium_verification_labels.csv\")\n",
    "print(\"Dataset\\n\", df.columns.tolist())\n",
    "print(df[\"Comment\"])\n",
    "print(df[\"Insult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2e073e79-7962-4f57-b8e6-7fd98f200faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment\n",
      " 0                      \"like this if you are a tribe fan\"\n",
      "1                   \"you're idiot.......................\"\n",
      "2       \"I am a woman Babs, and the only \"war on women...\n",
      "3       \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
      "4       \"haha green me red you now loser whos winning ...\n",
      "                              ...                        \n",
      "2230    \"FUCKIN LAME COME ON WTF STOP FUCKING OVER MY ...\n",
      "2231    \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...\n",
      "2232    \"sweetie pie is looking very much like her cou...\n",
      "2233    \"ball4real where are you with your miami g-ayn...\n",
      "2234    \"Man....if you are a 3 point shooter, you must...\n",
      "Name: Comment, Length: 2235, dtype: object\n",
      "Insult\n",
      " 0       0\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2230    0\n",
      "2231    1\n",
      "2232    0\n",
      "2233    1\n",
      "2234    0\n",
      "Name: Insult, Length: 2235, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "text_col=df['Comment']\n",
    "label_col=df['Insult']\n",
    "if text_col is None or label_col is None :\n",
    "    raise ValueError(\"Could not detect text/level columns\")\n",
    "#print(f\"Detected text column:\\n {text_col}\")    \n",
    "print('Comment\\n',text_col)\n",
    "print('Insult\\n',label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "88224174-6081-4e38-bdcd-db5294d18905",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[178]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mclean_comment\u001b[39m\u001b[33m'\u001b[39m]=df[\u001b[33m'\u001b[39m\u001b[33mComment\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[43mclean_text\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text():\n",
    "    text=re.sub(r'http\\S+|<.*?>','', str(text).lower()\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "df['clean_comment']=df['Comment'].apply(clean_text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8691b-4f73-4f81-9271-f38eb54b4d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
