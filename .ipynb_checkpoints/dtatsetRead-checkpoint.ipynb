{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6a7f14-9722-41a4-97d7-89727d672425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6b18ce-b470-4809-aa11-ada9fcf3a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jannatul Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5467da72-de06-433b-bf58-4148deb16d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "        index                                               Text  ed_label_0  \\\n",
      "0          0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
      "1          1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2          2    True or false, the situation as of March 200...    1.000000   \n",
      "3          3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4          4               This page will need disambiguation.     1.000000   \n",
      "...      ...                                                ...         ...   \n",
      "20078  20078    :IMO, Koenig is a perfectly good source and ...    0.947368   \n",
      "20079  20079    == Defamatory non-factual edit summaries == ...    0.666667   \n",
      "20080  20080    : I suggest you learn how those policy state...    0.900000   \n",
      "20081  20081    == Reverts ==  Do not pointlessly and needle...    1.000000   \n",
      "20082  20082   ::: No original synthesis was taking place - ...    0.750000   \n",
      "\n",
      "       ed_label_1  oh_label  \n",
      "0        0.100000         0  \n",
      "1        0.000000         0  \n",
      "2        0.000000         0  \n",
      "3        0.444444         0  \n",
      "4        0.000000         0  \n",
      "...           ...       ...  \n",
      "20078    0.052632         0  \n",
      "20079    0.333333         0  \n",
      "20080    0.100000         0  \n",
      "20081    0.000000         0  \n",
      "20082    0.250000         0  \n",
      "\n",
      "[20083 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Thesis\\cyberbullying.csv\")\n",
    "df.head()\n",
    "print(\"Dataset:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d04913-ddd7-4874-a277-a040d80703b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Next, maybe you could work on being less condescending with your suggestions about reading the naming conventions and FDL, both of which I read quite a while ago, thanks. I really liked the bit where you were explaining why you had no interest in fixing things I complained about because you felt insulted, yet you were being extremely insulting at the time. With any luck, you can learn to be less of a jerk. GregLindahl       \n",
      "2nd comment:\n",
      " `  :: the term ``standard model`` is itself less NPOV than I think we'd prefer...  :: if it's ``new-age speak`` then a lot of old-age people speak it - Karl Popper, the Pope, etc.  here's Karl Popper's view of this.  :: The clearest title for this article would be ``particle physics cosmology`` - but as I say that would require broader treatment of issues like the Anthropic Principle, cognitive bias beyond the particle physics zoo, etc.  :: as to accelerators, it's clear that while they are in use, someone is still looking for particles.  So this is not yet a settled ``cosmology`` so certain that we abandon the search... nor is it an arbitrary foundation ontology as you suggest, not subject to question.`\n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[3][\"Text\"]\n",
    "print(st)\n",
    "\n",
    "st=df.iloc[1][\"Text\"]\n",
    "print(\"2nd comment:\\n\",st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4706fd40-22a7-4606-99c6-26d802be1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['next', ',', 'maybe', 'you', 'could', 'work', 'on', 'being', 'less', 'condescending', 'with', 'your', 'suggestions', 'about', 'reading', 'the', 'naming', 'conventions', 'and', 'fdl', ',', 'both', 'of', 'which', 'i', 'read', 'quite', 'a', 'while', 'ago', ',', 'thanks', '.', 'i', 'really', 'liked', 'the', 'bit', 'where', 'you', 'were', 'explaining', 'why', 'you', 'had', 'no', 'interest', 'in', 'fixing', 'things', 'i', 'complained', 'about', 'because', 'you', 'felt', 'insulted', ',', 'yet', 'you', 'were', 'being', 'extremely', 'insulting', 'at', 'the', 'time', '.', 'with', 'any', 'luck', ',', 'you', 'can', 'learn', 'to', 'be', 'less', 'of', 'a', 'jerk', '.', 'greglindahl']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e033616-aa55-41ed-85b3-1f2e1b55572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This page will need disambiguation. \n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[4][\"Text\"]\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976939d5-4f03-4f46-8e13-142afefbd876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'page', 'will', 'need', 'disambiguation', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a62d58-0aaa-4f5c-9041-9dc797433af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  Important note for all sysops: There is a bug in the administrative move feature that truncates the moved history and changes the edit times.  Please do not use this feature until this bug is fixed. More information can be found in the talk of  and . Thank you. \n"
     ]
    }
   ],
   "source": [
    "ab=df.iloc[5][\"Text\"]\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15c18d7-21ec-4dbc-97ba-92212ef89aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', 'important', 'note', 'for', 'all', 'sysops', ':', 'there', 'is', 'a', 'bug', 'in', 'the', 'administrative', 'move', 'feature', 'that', 'truncates', 'the', 'moved', 'history', 'and', 'changes', 'the', 'edit', 'times', '.', 'please', 'do', 'not', 'use', 'this', 'feature', 'until', 'this', 'bug', 'is', 'fixed', '.', 'more', 'information', 'can', 'be', 'found', 'in', 'the', 'talk', 'of', 'and', '.', 'thank', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(ab.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529109d7-0a62-4712-bd19-2371735c3b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 41 samples and 53 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_disk=FreqDist(tokens)# it creates a dictionary list \n",
    "print(freq_disk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a3de95-373b-4129-b8eb-e87e9878e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "important1\n",
      "note1\n",
      "for1\n",
      "all1\n",
      "sysops1\n",
      ":1\n",
      "there1\n",
      "is2\n",
      "a1\n",
      "bug2\n",
      "in2\n",
      "the4\n",
      "administrative1\n",
      "move1\n",
      "feature2\n",
      "that1\n",
      "truncates1\n",
      "moved1\n",
      "history1\n",
      "and2\n",
      "changes1\n",
      "edit1\n",
      "times1\n",
      ".4\n",
      "please1\n",
      "do1\n",
      "not1\n",
      "use1\n",
      "this2\n",
      "until1\n",
      "fixed1\n",
      "more1\n",
      "information1\n",
      "can1\n",
      "be1\n",
      "found1\n",
      "talk1\n",
      "of1\n",
      "thank1\n",
      "you1\n"
     ]
    }
   ],
   "source": [
    "for word, freq in freq_disk.items():\n",
    "    print(word + str(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a9787f-d806-42dc-bcdd-638edae95abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I removed the following:  All names of early Polish rulers are ficticious and therefore this index naming Oda von Haldensleben and her husband Dagome records for the first time rulers of the Polanen tribe. Therefore it is indicated as being the first document of the later developing land named Poland.  This is quite a comment. All names are fictitious? It deserves at least some backing. \n"
     ]
    }
   ],
   "source": [
    "st=df.iloc[6][\"Text\"]\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bd6883-c9f6-4fcc-b6e0-95f2a994747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'removed', 'the', 'following', ':', 'all', 'names', 'of', 'early', 'polish', 'rulers', 'are', 'ficticious', 'and', 'therefore', 'this', 'index', 'naming', 'oda', 'von', 'haldensleben', 'and', 'her', 'husband', 'dagome', 'records', 'for', 'the', 'first', 'time', 'rulers', 'of', 'the', 'polanen', 'tribe', '.', 'therefore', 'it', 'is', 'indicated', 'as', 'being', 'the', 'first', 'document', 'of', 'the', 'later', 'developing', 'land', 'named', 'poland', '.', 'this', 'is', 'quite', 'a', 'comment', '.', 'all', 'names', 'are', 'fictitious', '?', 'it', 'deserves', 'at', 'least', 'some', 'backing', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(st.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32eef218-87a3-40de-bb41-b8f5c5daf82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 52 samples and 71 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist=FreqDist(tokens) #it creates frequency dictionary\n",
    "print(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7ed773-6d00-4d4c-9f6b-8a5fc4477ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 1\n",
      "removed 1\n",
      "the 5\n",
      "following 1\n",
      ": 1\n",
      "all 2\n",
      "names 2\n",
      "of 3\n",
      "early 1\n",
      "polish 1\n",
      "rulers 2\n",
      "are 2\n",
      "ficticious 1\n",
      "and 2\n",
      "therefore 2\n",
      "this 2\n",
      "index 1\n",
      "naming 1\n",
      "oda 1\n",
      "von 1\n",
      "haldensleben 1\n",
      "her 1\n",
      "husband 1\n",
      "dagome 1\n",
      "records 1\n",
      "for 1\n",
      "first 2\n",
      "time 1\n",
      "polanen 1\n",
      "tribe 1\n",
      ". 4\n",
      "it 2\n",
      "is 2\n",
      "indicated 1\n",
      "as 1\n",
      "being 1\n",
      "document 1\n",
      "later 1\n",
      "developing 1\n",
      "land 1\n",
      "named 1\n",
      "poland 1\n",
      "quite 1\n",
      "a 1\n",
      "comment 1\n",
      "fictitious 1\n",
      "? 1\n",
      "deserves 1\n",
      "at 1\n",
      "least 1\n",
      "some 1\n",
      "backing 1\n"
     ]
    }
   ],
   "source": [
    "#freq_Dist store dictionary and items returns the pair of data from the dictionary\n",
    "for word, freq in freq_dist.items(): \n",
    "    print(word , freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b631c567-7adc-44cb-a419-b886a0f48bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['removed', 'following', ':', 'names', 'early', 'polish', 'rulers', 'ficticious', 'therefore', 'index', 'naming', 'oda', 'von', 'haldensleben', 'husband', 'dagome', 'records', 'first', 'time', 'rulers', 'polanen', 'tribe', '.', 'therefore', 'indicated', 'first', 'document', 'later', 'developing', 'land', 'named', 'poland', '.', 'quite', 'comment', '.', 'names', 'fictitious', '?', 'deserves', 'least', 'backing', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [\n",
    "    word for word in tokens\n",
    "    if word not in stop_words\n",
    "]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1433e2fe-071e-49e5-bbea-b0046cb13207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'from', 'further', 'it', 'on', 'by', 'through', 'how', 'mightn', 'as', \"she'd\", 'doing', 'and', 'themselves', 'hadn', 'your', \"couldn't\", 'doesn', 'are', \"i'll\", 'below', 'only', 'shouldn', 'each', 'why', 'been', \"don't\", 'itself', 'same', 'that', 'they', 'were', 'about', 'very', 'his', 'myself', 'up', 'these', 'did', \"it'd\", 'after', 'ma', 'few', 'where', 'was', \"haven't\", 'should', 'so', 'until', 'am', 'me', 'our', \"should've\", 're', 'what', 'didn', 'their', 'such', 'in', 've', 'd', 'too', 'over', 'herself', \"they'd\", 'while', \"hadn't\", 'into', 'for', 'this', 'above', \"they'll\", 'weren', 'to', 'all', 'than', 'a', 'there', \"we'd\", 'under', 'wouldn', \"you'd\", 'don', 'some', 'because', \"mightn't\", 'o', 'y', 'she', 'who', 'had', 'he', 'ain', 'out', 'have', 'most', 'down', 'theirs', 'its', 'wasn', 's', 'aren', \"you'll\", 'won', 'mustn', 'needn', 'other', \"they're\", 'against', 'of', 'if', 'off', 'hasn', 'before', 'just', \"it'll\", 'nor', 'does', 'isn', 'm', 'my', 'couldn', \"she'll\", 'shan', 'or', \"that'll\", 'not', 'yourselves', \"hasn't\", \"shan't\", 'those', \"won't\", 'them', \"he'll\", 'do', \"isn't\", 'the', \"we've\", \"you've\", 'll', \"i'd\", 'any', 'again', 'can', 'when', 'an', \"wasn't\", 'at', 'has', 'whom', 'with', \"wouldn't\", \"mustn't\", 'more', 'here', \"shouldn't\", \"weren't\", \"you're\", 'him', \"he's\", 'no', 'yours', \"i'm\", 'once', \"they've\", \"he'd\", 'i', 'her', 'which', 'both', \"didn't\", \"doesn't\", \"i've\", 'hers', 'is', \"needn't\", 'will', 'between', 'having', 'being', 'ours', 'but', 'own', \"it's\", \"we'll\", 't', 'we', 'then', \"she's\", 'haven', \"we're\", 'ourselves', 'be', 'himself', 'you', 'during', \"aren't\", 'now'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "487aac46-6e2b-46e4-8906-ef17398301f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['removed', 'following', ':', 'names', 'early', 'polish', 'rulers', 'ficticious', 'therefore', 'index', 'naming', 'oda', 'von', 'haldensleben', 'husband', 'dagome', 'records', 'first', 'time', 'rulers', 'polanen', 'tribe', '.', 'therefore', 'indicated', 'first', 'document', 'later', 'developing', 'land', 'named', 'poland', '.', 'quite', 'comment', '.', 'names', 'fictitious', '?', 'deserves', 'least', 'backing', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []          # empty list to store valid words\n",
    "\n",
    "for word in tokens:           # loop through each token\n",
    "    if word not in stop_words:  # check if word is NOT a stop word\n",
    "        filtered_tokens.append(word)  # add word to the list\n",
    "print(filtered_tokens)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40c76404-a50c-4f24-a946-ec9c730df392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('removed', 'VBD'), ('the', 'DT'), ('following', 'NN'), (':', ':'), ('all', 'DT'), ('names', 'NNS'), ('of', 'IN'), ('early', 'JJ'), ('polish', 'JJ'), ('rulers', 'NNS'), ('are', 'VBP'), ('ficticious', 'JJ'), ('and', 'CC'), ('therefore', 'RB'), ('this', 'DT'), ('index', 'NN'), ('naming', 'VBG'), ('oda', 'PRP'), ('von', 'JJ'), ('haldensleben', 'NN'), ('and', 'CC'), ('her', 'PRP$'), ('husband', 'NN'), ('dagome', 'NN'), ('records', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('time', 'NN'), ('rulers', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('polanen', 'NN'), ('tribe', 'NN'), ('.', '.'), ('therefore', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('indicated', 'VBN'), ('as', 'IN'), ('being', 'VBG'), ('the', 'DT'), ('first', 'JJ'), ('document', 'NN'), ('of', 'IN'), ('the', 'DT'), ('later', 'JJ'), ('developing', 'VBG'), ('land', 'NN'), ('named', 'VBN'), ('poland', 'NN'), ('.', '.'), ('this', 'DT'), ('is', 'VBZ'), ('quite', 'RB'), ('a', 'DT'), ('comment', 'NN'), ('.', '.'), ('all', 'DT'), ('names', 'NNS'), ('are', 'VBP'), ('fictitious', 'JJ'), ('?', '.'), ('it', 'PRP'), ('deserves', 'VBZ'), ('at', 'IN'), ('least', 'JJS'), ('some', 'DT'), ('backing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f253b319-a16a-44cb-8a03-aaef4ed0b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i → i\n",
      "removed → removed\n",
      "the → the\n",
      "following → following\n",
      ": → :\n",
      "all → all\n",
      "names → name\n",
      "of → of\n",
      "early → early\n",
      "polish → polish\n",
      "rulers → ruler\n",
      "are → are\n",
      "ficticious → ficticious\n",
      "and → and\n",
      "therefore → therefore\n",
      "this → this\n",
      "index → index\n",
      "naming → naming\n",
      "oda → oda\n",
      "von → von\n",
      "haldensleben → haldensleben\n",
      "and → and\n",
      "her → her\n",
      "husband → husband\n",
      "dagome → dagome\n",
      "records → record\n",
      "for → for\n",
      "the → the\n",
      "first → first\n",
      "time → time\n",
      "rulers → ruler\n",
      "of → of\n",
      "the → the\n",
      "polanen → polanen\n",
      "tribe → tribe\n",
      ". → .\n",
      "therefore → therefore\n",
      "it → it\n",
      "is → is\n",
      "indicated → indicated\n",
      "as → a\n",
      "being → being\n",
      "the → the\n",
      "first → first\n",
      "document → document\n",
      "of → of\n",
      "the → the\n",
      "later → later\n",
      "developing → developing\n",
      "land → land\n",
      "named → named\n",
      "poland → poland\n",
      ". → .\n",
      "this → this\n",
      "is → is\n",
      "quite → quite\n",
      "a → a\n",
      "comment → comment\n",
      ". → .\n",
      "all → all\n",
      "names → name\n",
      "are → are\n",
      "fictitious → fictitious\n",
      "? → ?\n",
      "it → it\n",
      "deserves → deserves\n",
      "at → at\n",
      "least → least\n",
      "some → some\n",
      "backing → backing\n",
      ". → .\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for word in tokens:\n",
    "    base_word = lemmatizer.lemmatize(word)\n",
    "    print(word, \"→\", base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93268a9e-5446-4fd3-ae5a-c133aa813f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # verb\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))   # adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c73a199-f7e4-4ca3-8574-9d2d8290ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-i\n",
      "removed-remove\n",
      "the-the\n",
      "following-follow\n",
      ":-:\n",
      "all-all\n",
      "names-name\n",
      "of-of\n",
      "early-early\n",
      "polish-polish\n",
      "rulers-rulers\n",
      "are-be\n",
      "ficticious-ficticious\n",
      "and-and\n",
      "therefore-therefore\n",
      "this-this\n",
      "index-index\n",
      "naming-name\n",
      "oda-oda\n",
      "von-von\n",
      "haldensleben-haldensleben\n",
      "and-and\n",
      "her-her\n",
      "husband-husband\n",
      "dagome-dagome\n",
      "records-record\n",
      "for-for\n",
      "the-the\n",
      "first-first\n",
      "time-time\n",
      "rulers-rulers\n",
      "of-of\n",
      "the-the\n",
      "polanen-polanen\n",
      "tribe-tribe\n",
      ".-.\n",
      "therefore-therefore\n",
      "it-it\n",
      "is-be\n",
      "indicated-indicate\n",
      "as-as\n",
      "being-be\n",
      "the-the\n",
      "first-first\n",
      "document-document\n",
      "of-of\n",
      "the-the\n",
      "later-later\n",
      "developing-develop\n",
      "land-land\n",
      "named-name\n",
      "poland-poland\n",
      ".-.\n",
      "this-this\n",
      "is-be\n",
      "quite-quite\n",
      "a-a\n",
      "comment-comment\n",
      ".-.\n",
      "all-all\n",
      "names-name\n",
      "are-be\n",
      "fictitious-fictitious\n",
      "?-?\n",
      "it-it\n",
      "deserves-deserve\n",
      "at-at\n",
      "least-least\n",
      "some-some\n",
      "backing-back\n",
      ".-.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in tokens:\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    print (word+'-'+lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1620bd2-efff-4e48-ac7a-79a64c071c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i            POS=NN   → Base=i\n",
      "removed      POS=VBD  → Base=remove\n",
      "the          POS=DT   → Base=the\n",
      "following    POS=NN   → Base=following\n",
      ":            POS=:    → Base=:\n",
      "all          POS=DT   → Base=all\n",
      "names        POS=NNS  → Base=name\n",
      "of           POS=IN   → Base=of\n",
      "early        POS=JJ   → Base=early\n",
      "polish       POS=JJ   → Base=polish\n",
      "rulers       POS=NNS  → Base=ruler\n",
      "are          POS=VBP  → Base=be\n",
      "ficticious   POS=JJ   → Base=ficticious\n",
      "and          POS=CC   → Base=and\n",
      "therefore    POS=RB   → Base=therefore\n",
      "this         POS=DT   → Base=this\n",
      "index        POS=NN   → Base=index\n",
      "naming       POS=VBG  → Base=name\n",
      "oda          POS=PRP  → Base=oda\n",
      "von          POS=JJ   → Base=von\n",
      "haldensleben POS=NN   → Base=haldensleben\n",
      "and          POS=CC   → Base=and\n",
      "her          POS=PRP$ → Base=her\n",
      "husband      POS=NN   → Base=husband\n",
      "dagome       POS=NN   → Base=dagome\n",
      "records      POS=NNS  → Base=record\n",
      "for          POS=IN   → Base=for\n",
      "the          POS=DT   → Base=the\n",
      "first        POS=JJ   → Base=first\n",
      "time         POS=NN   → Base=time\n",
      "rulers       POS=NNS  → Base=ruler\n",
      "of           POS=IN   → Base=of\n",
      "the          POS=DT   → Base=the\n",
      "polanen      POS=NN   → Base=polanen\n",
      "tribe        POS=NN   → Base=tribe\n",
      ".            POS=.    → Base=.\n",
      "therefore    POS=IN   → Base=therefore\n",
      "it           POS=PRP  → Base=it\n",
      "is           POS=VBZ  → Base=be\n",
      "indicated    POS=VBN  → Base=indicate\n",
      "as           POS=IN   → Base=a\n",
      "being        POS=VBG  → Base=be\n",
      "the          POS=DT   → Base=the\n",
      "first        POS=JJ   → Base=first\n",
      "document     POS=NN   → Base=document\n",
      "of           POS=IN   → Base=of\n",
      "the          POS=DT   → Base=the\n",
      "later        POS=JJ   → Base=late\n",
      "developing   POS=VBG  → Base=develop\n",
      "land         POS=NN   → Base=land\n",
      "named        POS=VBN  → Base=name\n",
      "poland       POS=NN   → Base=poland\n",
      ".            POS=.    → Base=.\n",
      "this         POS=DT   → Base=this\n",
      "is           POS=VBZ  → Base=be\n",
      "quite        POS=RB   → Base=quite\n",
      "a            POS=DT   → Base=a\n",
      "comment      POS=NN   → Base=comment\n",
      ".            POS=.    → Base=.\n",
      "all          POS=DT   → Base=all\n",
      "names        POS=NNS  → Base=name\n",
      "are          POS=VBP  → Base=be\n",
      "fictitious   POS=JJ   → Base=fictitious\n",
      "?            POS=.    → Base=?\n",
      "it           POS=PRP  → Base=it\n",
      "deserves     POS=VBZ  → Base=deserve\n",
      "at           POS=IN   → Base=at\n",
      "least        POS=JJS  → Base=least\n",
      "some         POS=DT   → Base=some\n",
      "backing      POS=NN   → Base=backing\n",
      ".            POS=.    → Base=.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 1: Map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_pos):\n",
    "    if nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ      # adjective\n",
    "    elif nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB     # verb\n",
    "    elif nltk_pos.startswith('N'):\n",
    "        return wordnet.NOUN     # noun\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV      # adverb\n",
    "    else:\n",
    "        return wordnet.NOUN    # default fallback\n",
    "\n",
    "# Step 2: Your word list\n",
    "words = [\"running\", \"better\", \"cars\", \"studies\", \"quickly\", \"children\"]\n",
    "\n",
    "# Step 3: POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Step 4: Lemmatize using POS\n",
    "base_words = []\n",
    "\n",
    "for word, pos in pos_tags:\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    base_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "    base_words.append((word, pos, base_word))\n",
    "\n",
    "# Step 5: Print results\n",
    "for original, pos, base in base_words:\n",
    "    print(f\"{original:<12} POS={pos:<4} → Base={base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bd7f366-4b9d-4b77-9e0e-29f9c14a597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jannatul Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jannatul\n",
      "[nltk_data]     Mawya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "Detected TEXT column : Text\n",
      "Detected LABEL column: oh_label\n",
      "\n",
      "Logistic Regression\n",
      "Accuracy: 0.9235748070699527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      3562\n",
      "           1       0.75      0.49      0.59       455\n",
      "\n",
      "    accuracy                           0.92      4017\n",
      "   macro avg       0.84      0.73      0.78      4017\n",
      "weighted avg       0.92      0.92      0.92      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3487   75]\n",
      " [ 232  223]]\n",
      "\n",
      "Random Forest\n",
      "Accuracy: 0.9317898929549415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      3562\n",
      "           1       0.78      0.56      0.65       455\n",
      "\n",
      "    accuracy                           0.93      4017\n",
      "   macro avg       0.86      0.77      0.81      4017\n",
      "weighted avg       0.93      0.93      0.93      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3489   73]\n",
      " [ 201  254]]\n",
      "\n",
      "Naive Bayes\n",
      "Accuracy: 0.9168533731640528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95      3562\n",
      "           1       0.78      0.37      0.50       455\n",
      "\n",
      "    accuracy                           0.92      4017\n",
      "   macro avg       0.85      0.68      0.73      4017\n",
      "weighted avg       0.91      0.92      0.90      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3516   46]\n",
      " [ 288  167]]\n",
      "\n",
      "Voting Ensemble\n",
      "Accuracy: 0.932536718944486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      3562\n",
      "           1       0.80      0.54      0.65       455\n",
      "\n",
      "    accuracy                           0.93      4017\n",
      "   macro avg       0.87      0.76      0.80      4017\n",
      "weighted avg       0.93      0.93      0.93      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3499   63]\n",
      " [ 208  247]]\n",
      "\n",
      "AdaBoost\n",
      "Accuracy: 0.9121234752302714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      3562\n",
      "           1       0.63      0.56      0.59       455\n",
      "\n",
      "    accuracy                           0.91      4017\n",
      "   macro avg       0.78      0.76      0.77      4017\n",
      "weighted avg       0.91      0.91      0.91      4017\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3410  152]\n",
      " [ 201  254]]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Imports & NLTK setup\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------------\n",
    "# NLTK downloads (run once)\n",
    "# -------------------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize tools\n",
    "# -------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -------------------------------\n",
    "# POS tag mapping\n",
    "# -------------------------------\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# ===============================\n",
    "# Load dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\Jannatul Mawya\\OneDrive\\Pictures\\Thesis\\cyberbullying.csv\"\n",
    ")\n",
    "\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "\n",
    "# ===============================\n",
    "# Auto-detect text & label\n",
    "# ===============================\n",
    "text_col = None\n",
    "label_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object and text_col is None:\n",
    "        text_col = col\n",
    "    elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "        label_col = col\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise ValueError(\"Could not detect text/label columns\")\n",
    "\n",
    "print(f\"Detected TEXT column : {text_col}\")\n",
    "print(f\"Detected LABEL column: {label_col}\")\n",
    "\n",
    "df = df[[text_col, label_col]].copy()\n",
    "df.columns = ['comment', 'insult']\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df['insult'] = df['insult'].astype(int)\n",
    "\n",
    "# ===============================\n",
    "# Text preprocessing (LEMMATIZATION)\n",
    "# ===============================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|<.*?>', ' ', str(text).lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "df['clean_comment'] = df['comment'].apply(clean_text)\n",
    "\n",
    "# ===============================\n",
    "# Vectorization\n",
    "# ===============================\n",
    "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['clean_comment'])\n",
    "y = df['insult']\n",
    "\n",
    "# ===============================\n",
    "# Train-test split\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Models\n",
    "# ===============================\n",
    "lr = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "nb = MultinomialNB()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('rf', rf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    estimator=LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# Evaluation\n",
    "# ===============================\n",
    "for name, model in [\n",
    "    ('Logistic Regression', lr),\n",
    "    ('Random Forest', rf),\n",
    "    ('Naive Bayes', nb),\n",
    "    ('Voting Ensemble', voting),\n",
    "    ('AdaBoost', adaboost)\n",
    "]:\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1f044-2292-473f-bcf8-23a5f9cdc6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22857a-775d-472f-82e4-b523cfed726f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
